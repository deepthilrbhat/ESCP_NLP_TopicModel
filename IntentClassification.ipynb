{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowballstemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Intent DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./snipsdataset.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-831a1b410f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mintent_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./snipsdataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintent_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintent_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlm_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlm_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintent_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./snipsdataset.csv' does not exist"
     ]
    }
   ],
   "source": [
    "intent_dataset = pd.read_csv('./snipsdataset.csv')\n",
    "print(intent_dataset['intents'].unique())\n",
    "print(len(intent_dataset['intents'].unique()))\n",
    "lm_dataset = pd.DataFrame()\n",
    "lm_dataset[['text']] = intent_dataset[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split DataSet in Training Set / Testing Set in 80% / 20% (can be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_train_dataset, intent_test_dataset = train_test_split(intent_dataset, test_size=0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset :  3177\n",
      "Train Dataset :  12707\n",
      "Labels :  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Dataset : \" ,len(intent_test_dataset))\n",
    "print(\"Train Dataset : \", len(intent_train_dataset))\n",
    "print(\"Labels : \", len(intent_train_dataset.intents.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom RNN for Text Classification (with TFIDF embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Keras uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sentences from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = intent_train_dataset['text'].values\n",
    "test_sentences = intent_test_dataset['text'].values\n",
    "train_labels = intent_train_dataset['intents'].values\n",
    "test_labels = intent_test_dataset['intents'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "vocab_size = 15000\n",
    "batch_size = 1000\n",
    "top_words = 15000\n",
    "embedding_vector_length = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_matrix(train_sentences, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_sentences, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 50)                750050    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 357       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 750,407\n",
      "Trainable params: 750,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 11436 samples, validate on 1271 samples\n",
      "Epoch 1/10\n",
      "11436/11436 [==============================] - 3s 225us/step - loss: 1.7194 - acc: 0.5305 - val_loss: 1.3828 - val_acc: 0.8662\n",
      "Epoch 2/10\n",
      "11436/11436 [==============================] - 2s 186us/step - loss: 1.0309 - acc: 0.9357 - val_loss: 0.7952 - val_acc: 0.9315\n",
      "Epoch 3/10\n",
      "11436/11436 [==============================] - 2s 187us/step - loss: 0.5233 - acc: 0.9782 - val_loss: 0.4508 - val_acc: 0.9575\n",
      "Epoch 4/10\n",
      "11436/11436 [==============================] - 2s 183us/step - loss: 0.2736 - acc: 0.9890 - val_loss: 0.2924 - val_acc: 0.9662\n",
      "Epoch 5/10\n",
      "11436/11436 [==============================] - 2s 188us/step - loss: 0.1596 - acc: 0.9921 - val_loss: 0.2181 - val_acc: 0.9725\n",
      "Epoch 6/10\n",
      "11436/11436 [==============================] - 2s 189us/step - loss: 0.1067 - acc: 0.9948 - val_loss: 0.1796 - val_acc: 0.9740\n",
      "Epoch 7/10\n",
      "11436/11436 [==============================] - 2s 199us/step - loss: 0.0784 - acc: 0.9963 - val_loss: 0.1564 - val_acc: 0.9756\n",
      "Epoch 8/10\n",
      "11436/11436 [==============================] - 2s 184us/step - loss: 0.0604 - acc: 0.9972 - val_loss: 0.1413 - val_acc: 0.9764\n",
      "Epoch 9/10\n",
      "11436/11436 [==============================] - 2s 184us/step - loss: 0.0490 - acc: 0.9981 - val_loss: 0.1305 - val_acc: 0.9772\n",
      "Epoch 10/10\n",
      "11436/11436 [==============================] - 2s 190us/step - loss: 0.0408 - acc: 0.9980 - val_loss: 0.1223 - val_acc: 0.9772\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3177 [==============================] - 1s 223us/step\n",
      "Test accuracy: 0.9757632947696017\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    "\n",
    "Result = []\n",
    "for i in range(len(intent_test_dataset)):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    Result.append(predicted_label)\n",
    "    \n",
    "# Append the list of results to the test dataframe\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "intent_test_dataset['result_CustomRNN'] = Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GUILHAUMELeroy-Melin\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\stats.py:60: FutureWarning: supplying multiple axes to axis is deprecated and will be removed in a future version.\n",
      "  num = df[df > 1].dropna(axis=[0, 1], thresh=1).applymap(lambda n: choose(n, 2)).sum().sum() - np.float64(nis2 * njs2) / n2\n",
      "C:\\Users\\GUILHAUMELeroy-Melin\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:346: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return(np.float64(self.LRP) / self.LRN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted             AddToPlaylist  BookRestaurant  GetWeather  PlayMusic  \\\n",
      "Actual                                                                       \n",
      "AddToPlaylist                   444               0           0          3   \n",
      "BookRestaurant                    0             461           0          0   \n",
      "GetWeather                        0               3         459          0   \n",
      "PlayMusic                        19               0           1        381   \n",
      "RateBook                          0               1           0          0   \n",
      "SearchCreativeWork                3               0           1          6   \n",
      "SearchScreeningEvent              0               0           1          0   \n",
      "__all__                         466             465         462        390   \n",
      "\n",
      "Predicted             RateBook  SearchCreativeWork  SearchScreeningEvent  \\\n",
      "Actual                                                                     \n",
      "AddToPlaylist                0                   4                     0   \n",
      "BookRestaurant               0                   0                     0   \n",
      "GetWeather                   0                   1                     0   \n",
      "PlayMusic                    0                   9                     0   \n",
      "RateBook                   434                   0                     0   \n",
      "SearchCreativeWork           1                 448                     8   \n",
      "SearchScreeningEvent         0                  16                   473   \n",
      "__all__                    435                 478                   481   \n",
      "\n",
      "Predicted             __all__  \n",
      "Actual                         \n",
      "AddToPlaylist             451  \n",
      "BookRestaurant            461  \n",
      "GetWeather                463  \n",
      "PlayMusic                 410  \n",
      "RateBook                  435  \n",
      "SearchCreativeWork        467  \n",
      "SearchScreeningEvent      490  \n",
      "__all__                  3177  \n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.9757632987094743\n",
      "95% CI: (0.9698003463492605, 0.9808265555430609)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 0.0\n",
      "Kappa: 0.9717088914796843\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                               AddToPlaylist BookRestaurant  \\\n",
      "Population                                     3177           3177   \n",
      "P: Condition positive                           451            461   \n",
      "N: Condition negative                          2726           2716   \n",
      "Test outcome positive                           466            465   \n",
      "Test outcome negative                          2711           2712   \n",
      "TP: True Positive                               444            461   \n",
      "TN: True Negative                              2704           2712   \n",
      "FP: False Positive                               22              4   \n",
      "FN: False Negative                                7              0   \n",
      "TPR: (Sensitivity, hit rate, recall)       0.984479              1   \n",
      "TNR=SPC: (Specificity)                      0.99193       0.998527   \n",
      "PPV: Pos Pred Value (Precision)             0.95279       0.991398   \n",
      "NPV: Neg Pred Value                        0.997418              1   \n",
      "FPR: False-out                           0.00807043     0.00147275   \n",
      "FDR: False Discovery Rate                 0.0472103     0.00860215   \n",
      "FNR: Miss Rate                            0.0155211              0   \n",
      "ACC: Accuracy                              0.990872       0.998741   \n",
      "F1 score                                   0.968375        0.99568   \n",
      "MCC: Matthews correlation coefficient      0.963219       0.994956   \n",
      "Informedness                               0.976409       0.998527   \n",
      "Markedness                                 0.950208       0.991398   \n",
      "Prevalence                                 0.141958       0.145105   \n",
      "LR+: Positive likelihood ratio              121.986            679   \n",
      "LR-: Negative likelihood ratio            0.0156473              0   \n",
      "DOR: Diagnostic odds ratio                  7795.95            inf   \n",
      "FOR: False omission rate                 0.00258207              0   \n",
      "\n",
      "Classes                                GetWeather   PlayMusic     RateBook  \\\n",
      "Population                                   3177        3177         3177   \n",
      "P: Condition positive                         463         410          435   \n",
      "N: Condition negative                        2714        2767         2742   \n",
      "Test outcome positive                         462         390          435   \n",
      "Test outcome negative                        2715        2787         2742   \n",
      "TP: True Positive                             459         381          434   \n",
      "TN: True Negative                            2711        2758         2741   \n",
      "FP: False Positive                              3           9            1   \n",
      "FN: False Negative                              4          29            1   \n",
      "TPR: (Sensitivity, hit rate, recall)     0.991361    0.929268     0.997701   \n",
      "TNR=SPC: (Specificity)                   0.998895    0.996747     0.999635   \n",
      "PPV: Pos Pred Value (Precision)          0.993506    0.976923     0.997701   \n",
      "NPV: Neg Pred Value                      0.998527    0.989595     0.999635   \n",
      "FPR: False-out                         0.00110538  0.00325262  0.000364697   \n",
      "FDR: False Discovery Rate              0.00649351   0.0230769   0.00229885   \n",
      "FNR: Miss Rate                         0.00863931   0.0707317   0.00229885   \n",
      "ACC: Accuracy                            0.997797    0.988039      0.99937   \n",
      "F1 score                                 0.992432      0.9525     0.997701   \n",
      "MCC: Matthews correlation coefficient    0.991144     0.94605     0.997336   \n",
      "Informedness                             0.990255    0.926016     0.997336   \n",
      "Markedness                               0.992033    0.966518     0.997336   \n",
      "Prevalence                               0.145735    0.129053     0.136922   \n",
      "LR+: Positive likelihood ratio            896.851     285.698       2735.7   \n",
      "LR-: Negative likelihood ratio         0.00864887   0.0709625   0.00229969   \n",
      "DOR: Diagnostic odds ratio                 103696     4026.05  1.18959e+06   \n",
      "FOR: False omission rate                0.0014733   0.0104055  0.000364697   \n",
      "\n",
      "Classes                               SearchCreativeWork SearchScreeningEvent  \n",
      "Population                                          3177                 3177  \n",
      "P: Condition positive                                467                  490  \n",
      "N: Condition negative                               2710                 2687  \n",
      "Test outcome positive                                478                  481  \n",
      "Test outcome negative                               2699                 2696  \n",
      "TP: True Positive                                    448                  473  \n",
      "TN: True Negative                                   2680                 2679  \n",
      "FP: False Positive                                    30                    8  \n",
      "FN: False Negative                                    19                   17  \n",
      "TPR: (Sensitivity, hit rate, recall)            0.959315             0.965306  \n",
      "TNR=SPC: (Specificity)                           0.98893             0.997023  \n",
      "PPV: Pos Pred Value (Precision)                 0.937238             0.983368  \n",
      "NPV: Neg Pred Value                              0.99296             0.993694  \n",
      "FPR: False-out                                 0.0110701            0.0029773  \n",
      "FDR: False Discovery Rate                      0.0627615             0.016632  \n",
      "FNR: Miss Rate                                 0.0406852            0.0346939  \n",
      "ACC: Accuracy                                   0.984577             0.992131  \n",
      "F1 score                                        0.948148             0.974253  \n",
      "MCC: Matthews correlation coefficient           0.939178             0.969668  \n",
      "Informedness                                    0.948245             0.962329  \n",
      "Markedness                                      0.930199             0.977062  \n",
      "Prevalence                                      0.146994             0.154234  \n",
      "LR+: Positive likelihood ratio                   86.6581              324.222  \n",
      "LR-: Negative likelihood ratio                 0.0411407            0.0347975  \n",
      "DOR: Diagnostic odds ratio                       2106.39               9317.4  \n",
      "FOR: False omission rate                      0.00703964           0.00630564  \n"
     ]
    }
   ],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(intent_test_dataset['intents'].values, intent_test_dataset['result_CustomRNN'].values)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN for Text Classification (with Learned embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Concatenate, Dense, Conv2D, Reshape, MaxPool2D, Dropout, LSTM, Embedding, Bidirectional, GlobalAveragePooling1D, CuDNNLSTM, Input, Multiply, TimeDistributed, multiply, Flatten, RepeatVector, Permute, Lambda\n",
    "from keras.optimizers import Adamax, Adam\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Keras uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "vocab_size = 15000\n",
    "batch_size = 1000\n",
    "top_words = 15000\n",
    "embedding_vector_length = 300\n",
    "maxlen = 30\n",
    "\n",
    "## for CNN\n",
    "filter_sizes = [2,4,6]\n",
    "num_filters = 20\n",
    "drop = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowballstemmer\n",
    "import nltk\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "stemmer = snowballstemmer.EnglishStemmer\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "#stop.extend(lowfreq)\n",
    "toktok = nltk.tokenize.toktok.ToktokTokenizer()\n",
    "\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['text']\n",
    "intent_train_dataset['cleaned'].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].str.lower()\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].apply(toktok.tokenize)\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].apply(lambda x: [word for word in x if word not in stop])\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['text']\n",
    "intent_test_dataset['cleaned'].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].str.lower()\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].apply(toktok.tokenize)\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].apply(lambda x: [word for word in x if word not in stop])\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sentences from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = intent_train_dataset['cleaned'].values\n",
    "test_sentences = intent_test_dataset['cleaned'].values\n",
    "train_labels = intent_train_dataset['intents'].values\n",
    "test_labels = intent_test_dataset['intents'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12707, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "x_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GUILHAUMELeroy-Melin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 30, 300)      4500000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 30, 300, 1)   0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 29, 1, 20)    12020       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 27, 1, 20)    24020       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 1, 20)    36020       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 20)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 20)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 20)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 1, 20)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 60)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60)           0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 7)            427         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 7)            0           dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,572,487\n",
      "Trainable params: 4,572,487\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 11436 samples, validate on 1271 samples\n",
      "Epoch 1/25\n",
      "11436/11436 [==============================] - 6s 491us/step - loss: 1.8385 - acc: 0.4295 - val_loss: 1.6635 - val_acc: 0.9056\n",
      "Epoch 2/25\n",
      "11436/11436 [==============================] - 1s 112us/step - loss: 1.4714 - acc: 0.8486 - val_loss: 1.2272 - val_acc: 0.9559\n",
      "Epoch 3/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.9994 - acc: 0.9210 - val_loss: 0.7282 - val_acc: 0.9622\n",
      "Epoch 4/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.5910 - acc: 0.9467 - val_loss: 0.3868 - val_acc: 0.9646\n",
      "Epoch 5/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.3423 - acc: 0.9585 - val_loss: 0.2261 - val_acc: 0.9670\n",
      "Epoch 6/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.2226 - acc: 0.9658 - val_loss: 0.1592 - val_acc: 0.9725\n",
      "Epoch 7/25\n",
      "11436/11436 [==============================] - 1s 112us/step - loss: 0.1568 - acc: 0.9725 - val_loss: 0.1269 - val_acc: 0.9740\n",
      "Epoch 8/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.1214 - acc: 0.9787 - val_loss: 0.1105 - val_acc: 0.9732\n",
      "Epoch 9/25\n",
      "11436/11436 [==============================] - 1s 112us/step - loss: 0.0965 - acc: 0.9836 - val_loss: 0.1004 - val_acc: 0.9748\n",
      "Epoch 10/25\n",
      "11436/11436 [==============================] - 1s 112us/step - loss: 0.0831 - acc: 0.9860 - val_loss: 0.0921 - val_acc: 0.9764\n",
      "Epoch 11/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0696 - acc: 0.9878 - val_loss: 0.0885 - val_acc: 0.9764\n",
      "Epoch 12/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0598 - acc: 0.9900 - val_loss: 0.0846 - val_acc: 0.9780\n",
      "Epoch 13/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0503 - acc: 0.9914 - val_loss: 0.0808 - val_acc: 0.9788\n",
      "Epoch 14/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0476 - acc: 0.9923 - val_loss: 0.0787 - val_acc: 0.9795\n",
      "Epoch 15/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0408 - acc: 0.9927 - val_loss: 0.0791 - val_acc: 0.9795\n",
      "Epoch 16/25\n",
      "11436/11436 [==============================] - 1s 112us/step - loss: 0.0363 - acc: 0.9952 - val_loss: 0.0777 - val_acc: 0.9795\n",
      "Epoch 17/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0317 - acc: 0.9959 - val_loss: 0.0772 - val_acc: 0.9795\n",
      "Epoch 18/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0310 - acc: 0.9952 - val_loss: 0.0767 - val_acc: 0.9795\n",
      "Epoch 19/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0302 - acc: 0.9951 - val_loss: 0.0768 - val_acc: 0.9795\n",
      "Epoch 20/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0253 - acc: 0.9962 - val_loss: 0.0774 - val_acc: 0.9795\n",
      "Epoch 21/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0251 - acc: 0.9962 - val_loss: 0.0775 - val_acc: 0.9788\n",
      "Epoch 22/25\n",
      "11436/11436 [==============================] - 1s 112us/step - loss: 0.0222 - acc: 0.9972 - val_loss: 0.0770 - val_acc: 0.9788\n",
      "Epoch 23/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0190 - acc: 0.9976 - val_loss: 0.0758 - val_acc: 0.9795\n",
      "Epoch 24/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0186 - acc: 0.9974 - val_loss: 0.0769 - val_acc: 0.9795\n",
      "Epoch 25/25\n",
      "11436/11436 [==============================] - 1s 113us/step - loss: 0.0187 - acc: 0.9972 - val_loss: 0.0770 - val_acc: 0.9795\n"
     ]
    }
   ],
   "source": [
    "Inputs_W = Input(shape=(maxlen,))\n",
    "Inputs_E = Embedding(vocab_size, embedding_vector_length, input_length=maxlen)(Inputs_W)\n",
    "reshape = Reshape((maxlen,embedding_vector_length,1))(Inputs_E)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_vector_length), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_vector_length), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_vector_length), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "Dense_Label = Dense(num_labels)(dropout)\n",
    "Classifier = Activation('softmax')(Dense_Label)\n",
    "\n",
    "model = Model(input=Inputs_W, output=Classifier)\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0015, amsgrad=False)\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=25,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3177 [==============================] - 0s 46us/step\n",
      "Test accuracy: 0.9807995105450131\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    "\n",
    "Result = []\n",
    "for i in range(len(intent_test_dataset)):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    Result.append(predicted_label)\n",
    "    \n",
    "# Append the list of results to the test dataframe\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "intent_test_dataset['result_CustomRNN'] = Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(intent_test_dataset['ACM'].values, intent_test_dataset['result_CustomRNN'].values)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LSTM for Text Classification (with Learned Vector embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding, Bidirectional, GlobalAveragePooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Keras uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sentences from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = intent_train_dataset['text'].values\n",
    "test_sentences = intent_test_dataset['text'].values\n",
    "train_labels = intent_train_dataset['intents'].values\n",
    "test_labels = intent_test_dataset['intents'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "vocab_size = 20000\n",
    "batch_size = 100\n",
    "lstm_size = 256\n",
    "maxlen = 50\n",
    "embedding_vector_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "x_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 256)           5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 512)               1050624   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 7)                 3591      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 6,174,215\n",
      "Trainable params: 6,174,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 11436 samples, validate on 1271 samples\n",
      "Epoch 1/10\n",
      "11436/11436 [==============================] - 37s 3ms/step - loss: 0.6549 - acc: 0.8011 - val_loss: 0.1023 - val_acc: 0.9717\n",
      "Epoch 2/10\n",
      "11436/11436 [==============================] - 33s 3ms/step - loss: 0.0516 - acc: 0.9861 - val_loss: 0.0678 - val_acc: 0.9827\n",
      "Epoch 3/10\n",
      "11436/11436 [==============================] - 33s 3ms/step - loss: 0.0218 - acc: 0.9950 - val_loss: 0.0594 - val_acc: 0.9827\n",
      "Epoch 4/10\n",
      "11436/11436 [==============================] - 32s 3ms/step - loss: 0.0153 - acc: 0.9958 - val_loss: 0.0669 - val_acc: 0.9827\n",
      "Epoch 5/10\n",
      "11436/11436 [==============================] - 32s 3ms/step - loss: 0.0072 - acc: 0.9984 - val_loss: 0.0725 - val_acc: 0.9819\n",
      "Epoch 6/10\n",
      "11436/11436 [==============================] - 33s 3ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0921 - val_acc: 0.9772\n",
      "Epoch 7/10\n",
      "11436/11436 [==============================] - 32s 3ms/step - loss: 0.0083 - acc: 0.9983 - val_loss: 0.0859 - val_acc: 0.9795\n",
      "Epoch 8/10\n",
      "11436/11436 [==============================] - 32s 3ms/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0753 - val_acc: 0.9835\n",
      "Epoch 9/10\n",
      "11436/11436 [==============================] - 32s 3ms/step - loss: 7.0690e-04 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9811\n",
      "Epoch 10/10\n",
      "11436/11436 [==============================] - 32s 3ms/step - loss: 4.0218e-04 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9827\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3177 [==============================] - 3s 1ms/step\n",
      "Test accuracy: 0.982373317457949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-b5cde6f1e57a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintent_test_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mpredicted_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mResult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    "\n",
    "Result = []\n",
    "for i in range(len(intent_test_dataset)):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    Result.append(predicted_label)\n",
    "    \n",
    "# Append the list of results to the test dataframe\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "intent_test_dataset['result_CustomLSTM'] = Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(intent_test_dataset['ACM'].values, intent_test_dataset['result_CustomLSTM'].values)\n",
    "cm.print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
